{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We publish a lot of articles. Some of those headlines could be classified as “Clickbait” by\n",
    "distribution partners like Facebook or Google, which is bad for the performance of our articles.\n",
    "We want to provide an API to the business for them to enter the title of an article and get back\n",
    "the probability that article is either “News” or “Clickbait”. Thankfully, someone has already gone\n",
    "and determined what is “News” and “Clickbait” for us to train off of and that data, and an initial\n",
    "Python Flask app for the classifier, can be found HERE\n",
    "Build a model to predict how likely an article is to be clickbait. Provide some standard\n",
    "classification metrics for your work. Once you’ve done that, provide your model in an API\n",
    "endpoint for use by the business.\n",
    "\n",
    "Build a classifier (any classifier)\n",
    "Create an API for Text Inputs\n",
    "Create an API response with a clickbait pro\n",
    "Containerize Your Work\n",
    "\n",
    "Acceptance​ ​Criteria​:\n",
    "- The API Endpoint accepts strings between 1 and 150 characters\n",
    "- The API returns responses in .json format\n",
    "- The API returns the clickbait likelihood response as a float\n",
    "- There is a functional and reproducible API endpoint to provide text string inputs into\n",
    "\n",
    "which returns the probability that text entered into the API is “clickbait” or is “news”\n",
    "- Your classifier & API work is in a containerized environment (Docker, AWS Container\n",
    "Service, etc.)\n",
    "Considerations​:\n",
    "- What tests would you want to perform on the endpoint to ensure it can handle\n",
    "exceptions?\n",
    "- Extra Points: Deploying this on AWS or GCP on a live endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Detailed steps **\n",
    "\n",
    "- Load data and transform the json into a dataframe or list of titles\n",
    "- Use text tokenizer from sklearn to tokenize and create a data frame or sth\n",
    "- split your data into train and test\n",
    "- chose a model and train \n",
    "- save model\n",
    "- prepare a flask api\n",
    "- prepare the endpoints and functions\n",
    "- load model\n",
    "- setup a prediction method called by endpoint function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_matrix(X):\n",
    "    vec = TfidfVectorizer(stop_words='english', analyzer='word', lowercase=True, max_df=0.5, sublinear_tf=True)\n",
    "    return vec.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data_dir = \"./data/\"\n",
    "    files = []\n",
    "    fnames = ['buzzfeed.json', 'dose.json', 'clickhole.json', 'nytimes.json']\n",
    "    for fname in os.listdir(data_dir):\n",
    "        if fname in fnames:\n",
    "            with open(os.path.join(data_dir, fname)) as f:\n",
    "                files += [pd.DataFrame(json.loads(f.read()))[['article_title', 'clickbait']]]\n",
    "            \n",
    "    df = pd.concat(files)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    return df.article_title, df.clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = vectorize_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=30, max_features=2)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "pred_ytest = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_names = ['news', 'clickbait']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       news       0.92      0.97      0.95       619\n",
      "  clickbait       0.97      0.92      0.94       621\n",
      "\n",
      "avg / total       0.95      0.95      0.95      1240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=ytest, y_pred=pred_ytest, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cb_model_rf.pkl']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, filename=\"cb_model_rf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf2 = joblib.load(\"cb_model_rf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr2 = clf2.predict(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       news       1.00      1.00      1.00      2489\n",
      "  clickbait       1.00      1.00      1.00      2471\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=ytrain, y_pred=pr2, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
